{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Tutorial\n",
    "\n",
    "This notebook demonstrates how to train the unified DTA prediction system with 2-phase progressive training.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The training process consists of two phases:\n",
    "1. **Phase 1**: Train with frozen ESM-2 weights (faster convergence)\n",
    "2. **Phase 2**: Fine-tune ESM-2 layers for optimal performance\n",
    "\n",
    "We'll cover:\n",
    "- Setting up training configurations\n",
    "- Loading and preparing data\n",
    "- Running 2-phase training\n",
    "- Monitoring training progress\n",
    "- Evaluating trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import unified DTA system components\n",
    "from unified_dta.core.training import TrainingManager\n",
    "from unified_dta.core.models import ModelFactory\n",
    "from unified_dta.data.datasets import UnifiedDTADataset\n",
    "from unified_dta.core.evaluation import EvaluationMetrics\n",
    "from unified_dta.utils.config import load_config\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "We'll start by setting up training configurations. The system supports both lightweight and production configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'model_type': 'production',  # or 'lightweight' for faster training\n",
    "    'dataset': 'kiba',  # kiba, davis, or bindingdb\n",
    "    'batch_size': 4,  # Small batch size for memory efficiency\n",
    "    'num_epochs_phase1': 20,  # Phase 1: frozen ESM-2\n",
    "    'num_epochs_phase2': 10,  # Phase 2: fine-tuning\n",
    "    'learning_rate_phase1': 1e-3,\n",
    "    'learning_rate_phase2': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'early_stopping_patience': 5,\n",
    "    'checkpoint_dir': '../../checkpoints',\n",
    "    'log_dir': '../../logs',\n",
    "    'save_best_only': True,\n",
    "    'monitor_metric': 'val_loss'\n",
    "}\n",
    "\n",
    "# Model configuration\n",
    "if training_config['model_type'] == 'production':\n",
    "    model_config = {\n",
    "        'protein_encoder_type': 'esm',\n",
    "        'drug_encoder_type': 'gin',\n",
    "        'use_fusion': True,\n",
    "        'protein_config': {\n",
    "            'model_name': 'facebook/esm2_t6_8M_UR50D',\n",
    "            'output_dim': 128,\n",
    "            'max_length': 200,\n",
    "            'freeze_layers': True  # Will be unfrozen in phase 2\n",
    "        },\n",
    "        'drug_config': {\n",
    "            'output_dim': 128,\n",
    "            'num_layers': 5,\n",
    "            'hidden_dim': 256,\n",
    "            'dropout': 0.2\n",
    "        },\n",
    "        'fusion_config': {\n",
    "            'hidden_dim': 256,\n",
    "            'num_heads': 8,\n",
    "            'dropout': 0.1\n",
    "        },\n",
    "        'predictor_config': {\n",
    "            'hidden_dims': [512, 256],\n",
    "            'dropout': 0.3,\n",
    "            'activation': 'relu'\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    # Lightweight configuration for faster training/testing\n",
    "    model_config = {\n",
    "        'protein_encoder_type': 'cnn',\n",
    "        'drug_encoder_type': 'gin',\n",
    "        'use_fusion': False,\n",
    "        'protein_config': {\n",
    "            'output_dim': 64,\n",
    "            'num_filters': [32, 64],\n",
    "            'kernel_sizes': [3, 5, 7]\n",
    "        },\n",
    "        'drug_config': {\n",
    "            'output_dim': 64,\n",
    "            'num_layers': 3,\n",
    "            'hidden_dim': 128\n",
    "        },\n",
    "        'predictor_config': {\n",
    "            'hidden_dims': [128],\n",
    "            'dropout': 0.2\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"Training configuration: {training_config['model_type']}\")\n",
    "print(f\"Dataset: {training_config['dataset'].upper()}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Load the dataset and create data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = f\"../../data/{training_config['dataset']}\"\n",
    "print(f\"Loading {training_config['dataset'].upper()} dataset...\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = UnifiedDTADataset(\n",
    "    data_path=f\"{dataset_path}/train.csv\",\n",
    "    protein_encoder_type=model_config['protein_encoder_type'],\n",
    "    max_protein_length=model_config.get('protein_config', {}).get('max_length', 200)\n",
    ")\n",
    "\n",
    "val_dataset = UnifiedDTADataset(\n",
    "    data_path=f\"{dataset_path}/val.csv\",\n",
    "    protein_encoder_type=model_config['protein_encoder_type'],\n",
    "    max_protein_length=model_config.get('protein_config', {}).get('max_length', 200)\n",
    ")\n",
    "\n",
    "test_dataset = UnifiedDTADataset(\n",
    "    data_path=f\"{dataset_path}/test.csv\",\n",
    "    protein_encoder_type=model_config['protein_encoder_type'],\n",
    "    max_protein_length=model_config.get('protein_config', {}).get('max_length', 200)\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created successfully!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Creation\n",
    "\n",
    "Create the unified DTA model using the model factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model using factory\n",
    "print(\"Creating model...\")\n",
    "model = ModelFactory.create_model(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1e6:.1f} MB (float32)\")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "Set up the training manager with optimizers, schedulers, and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training manager\n",
    "training_manager = TrainingManager(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    checkpoint_dir=training_config['checkpoint_dir'],\n",
    "    log_dir=training_config['log_dir']\n",
    ")\n",
    "\n",
    "# Set up loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Set up optimizers for both phases\n",
    "optimizer_phase1 = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=training_config['learning_rate_phase1'],\n",
    "    weight_decay=training_config['weight_decay']\n",
    ")\n",
    "\n",
    "optimizer_phase2 = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=training_config['learning_rate_phase2'],\n",
    "    weight_decay=training_config['weight_decay']\n",
    ")\n",
    "\n",
    "# Set up learning rate schedulers\n",
    "scheduler_phase1 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_phase1, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scheduler_phase2 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_phase2, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Phase 1 LR: {training_config['learning_rate_phase1']}\")\n",
    "print(f\"Phase 2 LR: {training_config['learning_rate_phase2']}\")\n",
    "print(f\"Weight decay: {training_config['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 1 Training: Frozen ESM-2\n",
    "\n",
    "Train the model with frozen ESM-2 weights to quickly learn drug-protein interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1 TRAINING: Frozen ESM-2 Weights\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure ESM-2 weights are frozen (if using ESM encoder)\n",
    "if hasattr(model, 'protein_encoder') and hasattr(model.protein_encoder, 'freeze_layers'):\n",
    "    model.protein_encoder.freeze_layers()\n",
    "    print(\"ESM-2 layers frozen for Phase 1\")\n",
    "\n",
    "# Training history\n",
    "phase1_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_rmse': [],\n",
    "    'val_pearson': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(training_config['num_epochs_phase1']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{training_config['num_epochs_phase1']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch in train_pbar:\n",
    "        # Move batch to device\n",
    "        drug_data = batch['drug'].to(device)\n",
    "        protein_data = batch['protein']\n",
    "        if isinstance(protein_data, torch.Tensor):\n",
    "            protein_data = protein_data.to(device)\n",
    "        elif isinstance(protein_data, list):\n",
    "            # ESM-2 expects list of sequences\n",
    "            pass\n",
    "        affinity = batch['affinity'].to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer_phase1.zero_grad()\n",
    "        predictions = model(drug_data, protein_data)\n",
    "        loss = criterion(predictions.squeeze(), affinity)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer_phase1.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        \n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "        for batch in val_pbar:\n",
    "            drug_data = batch['drug'].to(device)\n",
    "            protein_data = batch['protein']\n",
    "            if isinstance(protein_data, torch.Tensor):\n",
    "                protein_data = protein_data.to(device)\n",
    "            affinity = batch['affinity'].to(device).float()\n",
    "            \n",
    "            predictions = model(drug_data, protein_data)\n",
    "            loss = criterion(predictions.squeeze(), affinity)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "            \n",
    "            val_predictions.extend(predictions.squeeze().cpu().numpy())\n",
    "            val_targets.extend(affinity.cpu().numpy())\n",
    "            \n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = np.array(val_predictions)\n",
    "    val_targets = np.array(val_targets)\n",
    "    \n",
    "    val_rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))\n",
    "    val_pearson = np.corrcoef(val_predictions, val_targets)[0, 1]\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler_phase1.step(avg_val_loss)\n",
    "    current_lr = optimizer_phase1.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    phase1_history['train_loss'].append(avg_train_loss)\n",
    "    phase1_history['val_loss'].append(avg_val_loss)\n",
    "    phase1_history['val_rmse'].append(val_rmse)\n",
    "    phase1_history['val_pearson'].append(val_pearson)\n",
    "    phase1_history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Val RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Val Pearson: {val_pearson:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping and checkpointing\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        if training_config['save_best_only']:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_phase1.state_dict(),\n",
    "                'scheduler_state_dict': scheduler_phase1.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'model_config': model_config,\n",
    "                'training_config': training_config\n",
    "            }\n",
    "            \n",
    "            os.makedirs(training_config['checkpoint_dir'], exist_ok=True)\n",
    "            torch.save(checkpoint, f\"{training_config['checkpoint_dir']}/best_model_phase1.pt\")\n",
    "            print(f\"✓ Best model saved (Val Loss: {avg_val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    # Early stopping\n",
    "    if patience_counter >= training_config['early_stopping_patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nPhase 1 training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 
 },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase 2 Training: ESM-2 Fine-tuning\n",
    "\n",
    "Fine-tune the last layers of ESM-2 for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 TRAINING: ESM-2 Fine-tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best model from Phase 1\n",
    "if os.path.exists(f\"{training_config['checkpoint_dir']}/best_model_phase1.pt\"):\n",
    "    checkpoint = torch.load(f\"{training_config['checkpoint_dir']}/best_model_phase1.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded best model from Phase 1\")\n",
    "\n",
    "# Unfreeze ESM-2 layers for fine-tuning (if using ESM encoder)\n",
    "if hasattr(model, 'protein_encoder') and hasattr(model.protein_encoder, 'unfreeze_layers'):\n",
    "    model.protein_encoder.unfreeze_layers(num_layers=4)  # Unfreeze last 4 layers\n",
    "    print(\"ESM-2 last 4 layers unfrozen for Phase 2\")\n",
    "\n",
    "# Update trainable parameters count\n",
    "trainable_params_phase2 = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters in Phase 2: {trainable_params_phase2:,}\")\n",
    "\n",
    "# Training history for Phase 2\n",
    "phase2_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_rmse': [],\n",
    "    'val_pearson': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "best_val_loss_phase2 = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(training_config['num_epochs_phase2']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{training_config['num_epochs_phase2']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch in train_pbar:\n",
    "        drug_data = batch['drug'].to(device)\n",
    "        protein_data = batch['protein']\n",
    "        if isinstance(protein_data, torch.Tensor):\n",
    "            protein_data = protein_data.to(device)\n",
    "        affinity = batch['affinity'].to(device).float()\n",
    "        \n",
    "        optimizer_phase2.zero_grad()\n",
    "        predictions = model(drug_data, protein_data)\n",
    "        loss = criterion(predictions.squeeze(), affinity)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer_phase2.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        \n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "        for batch in val_pbar:\n",
    "            drug_data = batch['drug'].to(device)\n",
    "            protein_data = batch['protein']\n",
    "            if isinstance(protein_data, torch.Tensor):\n",
    "                protein_data = protein_data.to(device)\n",
    "            affinity = batch['affinity'].to(device).float()\n",
    "            \n",
    "            predictions = model(drug_data, protein_data)\n",
    "            loss = criterion(predictions.squeeze(), affinity)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "            \n",
    "            val_predictions.extend(predictions.squeeze().cpu().numpy())\n",
    "            val_targets.extend(affinity.cpu().numpy())\n",
    "            \n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_predictions = np.array(val_predictions)\n",
    "    val_targets = np.array(val_targets)\n",
    "    \n",
    "    val_rmse = np.sqrt(np.mean((val_predictions - val_targets) ** 2))\n",
    "    val_pearson = np.corrcoef(val_predictions, val_targets)[0, 1]\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler_phase2.step(avg_val_loss)\n",
    "    current_lr = optimizer_phase2.param_groups[0]['lr']\n",
    "    \n",
    "    # Store history\n",
    "    phase2_history['train_loss'].append(avg_train_loss)\n",
    "    phase2_history['val_loss'].append(avg_val_loss)\n",
    "    phase2_history['val_rmse'].append(val_rmse)\n",
    "    phase2_history['val_pearson'].append(val_pearson)\n",
    "    phase2_history['learning_rate'].append(current_lr)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Val RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Val Pearson: {val_pearson:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping and checkpointing\n",
    "    if avg_val_loss < best_val_loss_phase2:\n",
    "        best_val_loss_phase2 = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        if training_config['save_best_only']:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_phase2.state_dict(),\n",
    "                'scheduler_state_dict': scheduler_phase2.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'model_config': model_config,\n",
    "                'training_config': training_config,\n",
    "                'phase1_history': phase1_history,\n",
    "                'phase2_history': phase2_history\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f\"{training_config['checkpoint_dir']}/best_model_final.pt\")\n",
    "            print(f\"✓ Best final model saved (Val Loss: {avg_val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    # Early stopping\n",
    "    if patience_counter >= training_config['early_stopping_patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nPhase 2 training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss_phase2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization\n",
    "\n",
    "Visualize the training progress for both phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Progress: 2-Phase Progressive Training', fontsize=16)\n",
    "\n",
    "# Combine histories for plotting\n",
    "total_epochs_phase1 = len(phase1_history['train_loss'])\n",
    "total_epochs_phase2 = len(phase2_history['train_loss'])\n",
    "\n",
    "all_train_loss = phase1_history['train_loss'] + phase2_history['train_loss']\n",
    "all_val_loss = phase1_history['val_loss'] + phase2_history['val_loss']\n",
    "all_val_rmse = phase1_history['val_rmse'] + phase2_history['val_rmse']\n",
    "all_val_pearson = phase1_history['val_pearson'] + phase2_history['val_pearson']\n",
    "\n",
    "epochs = list(range(1, len(all_train_loss) + 1))\n",
    "phase_boundary = total_epochs_phase1\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0, 0].plot(epochs, all_train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
    "axes[0, 0].plot(epochs, all_val_loss, 'r-', label='Val Loss', alpha=0.7)\n",
    "axes[0, 0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Phase 1→2')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: RMSE\n",
    "axes[0, 1].plot(epochs, all_val_rmse, 'g-', label='Val RMSE', alpha=0.7)\n",
    "axes[0, 1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Phase 1→2')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('Validation RMSE')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Pearson Correlation\n",
    "axes[1, 0].plot(epochs, all_val_pearson, 'm-', label='Val Pearson', alpha=0.7)\n",
    "axes[1, 0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Phase 1→2')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Pearson Correlation')\n",
    "axes[1, 0].set_title('Validation Pearson Correlation')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning Rate\n",
    "all_lr = phase1_history['learning_rate'] + phase2_history['learning_rate']\n",
    "axes[1, 1].plot(epochs, all_lr, 'orange', label='Learning Rate', alpha=0.7)\n",
    "axes[1, 1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Phase 1→2')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Phase 1 Epochs: {total_epochs_phase1}\")\n",
    "print(f\"Phase 2 Epochs: {total_epochs_phase2}\")\n",
    "print(f\"Total Epochs: {total_epochs_phase1 + total_epochs_phase2}\")\n",
    "print(f\"\\nBest Phase 1 Val Loss: {min(phase1_history['val_loss']):.4f}\")\n",
    "print(f\"Best Phase 2 Val Loss: {min(phase2_history['val_loss']):.4f}\")\n",
    "print(f\"Final Val RMSE: {all_val_rmse[-1]:.4f}\")\n",
    "print(f\"Final Val Pearson: {all_val_pearson[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the final trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best final model\n",
    "if os.path.exists(f\"{training_config['checkpoint_dir']}/best_model_final.pt\"):\n",
    "    checkpoint = torch.load(f\"{training_config['checkpoint_dir']}/best_model_final.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded best final model for evaluation\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "test_loss = 0.0\n",
    "test_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for batch in test_pbar:\n",
    "        drug_data = batch['drug'].to(device)\n",
    "        protein_data = batch['protein']\n",
    "        if isinstance(protein_data, torch.Tensor):\n",
    "            protein_data = protein_data.to(device)\n",
    "        affinity = batch['affinity'].to(device).float()\n",
    "        \n",
    "        predictions = model(drug_data, protein_data)\n",
    "        loss = criterion(predictions.squeeze(), affinity)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_batches += 1\n",
    "        \n",
    "        test_predictions.extend(predictions.squeeze().cpu().numpy())\n",
    "        test_targets.extend(affinity.cpu().numpy())\n",
    "        \n",
    "        test_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "# Calculate comprehensive test metrics\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "test_mse = np.mean((test_predictions - test_targets) ** 2)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = np.mean(np.abs(test_predictions - test_targets))\n",
    "test_pearson = np.corrcoef(test_predictions, test_targets)[0, 1]\n",
    "\n",
    "# Spearman correlation\n",
    "from scipy.stats import spearmanr\n",
    "test_spearman, _ = spearmanr(test_predictions, test_targets)\n",
    "\n",
    "# Concordance Index (CI)\n",
    "def concordance_index(y_true, y_pred):\n",
    "    \"\"\"Calculate concordance index (C-index)\"\"\"\n",
    "    n = len(y_true)\n",
    "    concordant = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if y_true[i] != y_true[j]:\n",
    "                total_pairs += 1\n",
    "                if (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]) or \\\n",
    "                   (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]):\n",
    "                    concordant += 1\n",
    "    \n",
    "    return concordant / total_pairs if total_pairs > 0 else 0.0\n",
    "\n",
    "test_ci = concordance_index(test_targets, test_predictions)\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"Test Loss: {test_loss / test_batches:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test Pearson: {test_pearson:.4f}\")\n",
    "print(f\"Test Spearman: {test_spearman:.4f}\")\n",
    "print(f\"Test CI: {test_ci:.4f}\")\n",
    "\n",
    "# Create evaluation plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[0].scatter(test_targets, test_predictions, alpha=0.6, s=20)\n",
    "axes[0].plot([test_targets.min(), test_targets.max()], \n",
    "             [test_targets.min(), test_targets.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Affinity')\n",
    "axes[0].set_ylabel('Predicted Affinity')\n",
    "axes[0].set_title(f'Predicted vs Actual\\n(Pearson: {test_pearson:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = test_predictions - test_targets\n",
    "axes[1].scatter(test_targets, residuals, alpha=0.6, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Actual Affinity')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title(f'Residual Plot\\n(RMSE: {test_rmse:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation results\n",
    "evaluation_results = {\n",
    "    'test_loss': test_loss / test_batches,\n",
    "    'test_mse': test_mse,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_pearson': test_pearson,\n",
    "    'test_spearman': test_spearman,\n",
    "    'test_ci': test_ci,\n",
    "    'model_config': model_config,\n",
    "    'training_config': training_config\n",
    "}\n",
    "\n",
    "# Save results to file\n",
    "import json\n",
    "os.makedirs(training_config['log_dir'], exist_ok=True)\n",
    "with open(f\"{training_config['log_dir']}/evaluation_results.json\", 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to {training_config['log_dir']}/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Usage Analysis\n",
    "\n",
    "Analyze memory usage during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MEMORY USAGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model memory footprint\n",
    "model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6\n",
    "print(f\"Model size: {model_size_mb:.1f} MB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # GPU memory usage\n",
    "    gpu_memory_allocated = torch.cuda.memory_allocated() / 1e6\n",
    "    gpu_memory_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e6\n",
    "    \n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"Allocated: {gpu_memory_allocated:.1f} MB\")\n",
    "    print(f\"Reserved: {gpu_memory_reserved:.1f} MB\")\n",
    "    print(f\"Total: {gpu_memory_total:.1f} MB\")\n",
    "    print(f\"Usage: {gpu_memory_allocated/gpu_memory_total*100:.1f}%\")\n",
    "    \n",
    "    # Memory efficiency tips\n",
    "    print(f\"\\nMemory Optimization Tips:\")\n",
    "    if gpu_memory_allocated > gpu_memory_total * 0.8:\n",
    "        print(\"⚠️  High memory usage detected. Consider:\")\n",
    "        print(\"   - Reducing batch size\")\n",
    "        print(\"   - Using gradient checkpointing\")\n",
    "        print(\"   - Truncating protein sequences further\")\n",
    "    else:\n",
    "        print(\"✓ Memory usage is within acceptable limits\")\n",
    "        if gpu_memory_allocated < gpu_memory_total * 0.5:\n",
    "            print(\"💡 You could potentially increase batch size for faster training\")\n",
    "\n",
    "# Batch size recommendations\n",
    "print(f\"\\nBatch Size Recommendations:\")\n",
    "current_batch_size = training_config['batch_size']\n",
    "print(f\"Current batch size: {current_batch_size}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    memory_per_sample = gpu_memory_allocated / current_batch_size\n",
    "    max_batch_size = int(gpu_memory_total * 0.8 / memory_per_sample)\n",
    "    print(f\"Estimated max batch size: {max_batch_size}\")\n",
    "    \n",
    "    if max_batch_size > current_batch_size * 2:\n",
    "        recommended_batch_size = min(current_batch_size * 2, max_batch_size)\n",
    "        print(f\"💡 Recommended batch size: {recommended_batch_size}\")\n",
    "else:\n",
    "    print(\"Running on CPU - consider smaller batch sizes for memory efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Inference Example\n",
    "\n",
    "Demonstrate how to use the trained model for inference on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL INFERENCE EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example drug-protein pairs for inference\n",
    "example_pairs = [\n",
    "    {\n",
    "        'smiles': 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen\n",
    "        'protein_sequence': 'MKKFFDSRREQGGSGLGSGSSGGGGSGGGYIGSR',  # Short example sequence\n",
    "        'description': 'Ibuprofen with example protein'\n",
    "    },\n",
    "    {\n",
    "        'smiles': 'CC1=CC=C(C=C1)C(=O)C2=CC=CC=C2',  # Simple aromatic compound\n",
    "        'protein_sequence': 'MKWVTFISLLLLFSSAYSRGVFRRDTHKSEIAHRFKDLGE',  # Another example\n",
    "        'description': 'Aromatic compound with protein fragment'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to predict affinity for new pairs\n",
    "def predict_affinity(model, smiles, protein_sequence, device):\n",
    "    \"\"\"Predict binding affinity for a drug-protein pair\"\"\"\n",
    "    from unified_dta.data.preprocessing import smiles_to_graph, validate_smiles\n",
    "    \n",
    "    # Validate and process SMILES\n",
    "    if not validate_smiles(smiles):\n",
    "        raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "    \n",
    "    # Convert SMILES to graph\n",
    "    drug_graph = smiles_to_graph(smiles)\n",
    "    if drug_graph is None:\n",
    "        raise ValueError(f\"Could not convert SMILES to graph: {smiles}\")\n",
    "    \n",
    "    # Prepare protein data\n",
    "    if model_config['protein_encoder_type'] == 'esm':\n",
    "        # ESM expects list of sequences\n",
    "        protein_data = [protein_sequence]\n",
    "    else:\n",
    "        # CNN expects tokenized sequences\n",
    "        from unified_dta.data.preprocessing import tokenize_protein\n",
    "        protein_data = tokenize_protein(protein_sequence)\n",
    "        protein_data = torch.tensor(protein_data).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Move drug graph to device\n",
    "    drug_graph = drug_graph.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(drug_graph.unsqueeze(0), protein_data)\n",
    "        return prediction.item()\n",
    "\n",
    "# Make predictions for example pairs\n",
    "print(\"Making predictions for example drug-protein pairs...\\n\")\n",
    "\n",
    "for i, pair in enumerate(example_pairs, 1):\n",
    "    try:\n",
    "        prediction = predict_affinity(\n",
    "            model, \n",
    "            pair['smiles'], \n",
    "            pair['protein_sequence'], \n",
    "            device\n",
    "        )\n",
    "        \n",
    "        print(f\"Example {i}: {pair['description']}\")\n",
    "        print(f\"SMILES: {pair['smiles']}\")\n",
    "        print(f\"Protein: {pair['protein_sequence'][:50]}{'...' if len(pair['protein_sequence']) > 50 else ''}\")\n",
    "        print(f\"Predicted Affinity: {prediction:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for example {i}: {str(e)}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n💡 Inference Tips:\")\n",
    "print(\"- Ensure SMILES strings are valid before prediction\")\n",
    "print(\"- Protein sequences are automatically truncated to max_length\")\n",
    "print(\"- Batch multiple predictions for better efficiency\")\n",
    "print(\"- Use model.eval() mode for inference\")\n",
    "print(\"- Consider using torch.no_grad() to save memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Summary and Next Steps\n",
    "\n",
    "Summary of the training process and recommendations for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n📊 Training Summary:\")\n",
    "print(f\"Model Type: {training_config['model_type'].title()}\")\n",
    "print(f\"Dataset: {training_config['dataset'].upper()}\")\n",
    "print(f\"Total Training Time: Phase 1 + Phase 2\")\n",
    "print(f\"Final Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Final Test Pearson: {test_pearson:.4f}\")\n",
    "print(f\"Model Size: {model_size_mb:.1f} MB\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\n📈 Performance Analysis:\")\n",
    "if test_pearson > 0.8:\n",
    "    print(\"✅ Excellent correlation achieved!\")\n",
    "elif test_pearson > 0.6:\n",
    "    print(\"✅ Good correlation achieved\")\n",
    "elif test_pearson > 0.4:\n",
    "    print(\"⚠️  Moderate correlation - consider improvements\")\n",
    "else:\n",
    "    print(\"❌ Low correlation - model needs significant improvements\")\n",
    "\n",
    "if test_rmse < 0.5:\n",
    "    print(\"✅ Low RMSE - good prediction accuracy\")\n",
    "elif test_rmse < 1.0:\n",
    "    print(\"✅ Reasonable RMSE\")\n",
    "else:\n",
    "    print(\"⚠️  High RMSE - consider model improvements\")\n",
    "\n",
    "# Recommendations for improvement\n",
    "print(\"\\n🚀 Recommendations for Further Improvement:\")\n",
    "\n",
    "print(\"\\n1. Model Architecture:\")\n",
    "if training_config['model_type'] == 'lightweight':\n",
    "    print(\"   - Try the production model with ESM-2 encoder\")\n",
    "    print(\"   - Enable fusion mechanisms for better feature combination\")\n",
    "else:\n",
    "    print(\"   - Experiment with different fusion architectures\")\n",
    "    print(\"   - Try larger ESM-2 models (if memory allows)\")\n",
    "    print(\"   - Add attention mechanisms between drug and protein features\")\n",
    "\n",
    "print(\"\\n2. Training Strategy:\")\n",
    "print(\"   - Experiment with different phase transition points\")\n",
    "print(\"   - Try different learning rate schedules\")\n",
    "print(\"   - Implement curriculum learning (easy to hard samples)\")\n",
    "print(\"   - Add data augmentation techniques\")\n",
    "\n",
    "print(\"\\n3. Data Improvements:\")\n",
    "print(\"   - Combine multiple datasets (KIBA + Davis + BindingDB)\")\n",
    "print(\"   - Add molecular descriptors as additional features\")\n",
    "print(\"   - Implement cross-validation for robust evaluation\")\n",
    "print(\"   - Filter out low-quality data points\")\n",
    "\n",
    "print(\"\\n4. Regularization:\")\n",
    "print(\"   - Experiment with different dropout rates\")\n",
    "print(\"   - Add weight decay to prevent overfitting\")\n",
    "print(\"   - Implement early stopping with patience\")\n",
    "print(\"   - Try label smoothing for regression\")\n",
    "\n",
    "print(\"\\n5. Evaluation:\")\n",
    "print(\"   - Implement k-fold cross-validation\")\n",
    "print(\"   - Compare against baseline models\")\n",
    "print(\"   - Analyze predictions by protein family\")\n",
    "print(\"   - Create confidence intervals for predictions\")\n",
    "\n",
    "# Save training configuration for reproducibility\n",
    "final_config = {\n",
    "    'model_config': model_config,\n",
    "    'training_config': training_config,\n",
    "    'final_metrics': {\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_pearson': test_pearson,\n",
    "        'test_spearman': test_spearman,\n",
    "        'test_ci': test_ci\n",
    "    },\n",
    "    'model_info': {\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params_phase2,\n",
    "        'model_size_mb': model_size_mb\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{training_config['log_dir']}/final_training_config.json\", 'w') as f:\n",
    "    json.dump(final_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Training configuration saved to {training_config['log_dir']}/final_training_config.json\")\n",
    "print(f\"💾 Model checkpoint saved to {training_config['checkpoint_dir']}/best_model_final.pt\")\n",
    "\n",
    "print(\"\\n🎉 Training completed successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use the trained model for drug-target affinity predictions\")\n",
    "print(\"2. Experiment with the recommendations above\")\n",
    "print(\"3. Deploy the model for production use\")\n",
    "print(\"4. Explore drug generation capabilities (optional)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}