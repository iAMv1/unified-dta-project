{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preparation Tutorial\n",
                "\n",
                "This notebook demonstrates how to prepare and preprocess data for the Unified DTA System.\n",
                "\n",
                "## What you'll learn:\n",
                "1. Load and validate datasets (KIBA, Davis, BindingDB)\n",
                "2. Preprocess SMILES strings and protein sequences\n",
                "3. Handle data quality issues\n",
                "4. Create custom datasets\n",
                "5. Data augmentation techniques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run if needed)\n",
                "# !pip install unified-dta rdkit-pypi torch-geometric transformers pandas matplotlib seaborn\n",
                "\n",
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from rdkit import Chem\n",
                "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add project root to path (for development)\n",
                "sys.path.insert(0, os.path.join(os.getcwd(), '../..'))\n",
                "\n",
                "from unified_dta.data import DTADataset, DataProcessor\n",
                "from unified_dta.utils import validate_smiles, validate_protein_sequence\n",
                "\n",
                "print(\"Environment setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Loading Standard Datasets\n",
                "\n",
                "The system supports three major DTA datasets: KIBA, Davis, and BindingDB."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load sample datasets\n",
                "datasets = {}\n",
                "dataset_names = ['kiba', 'davis', 'bindingdb']\n",
                "\n",
                "for name in dataset_names:\n",
                "    try:\n",
                "        train_path = f'../../data/{name}_train.csv'\n",
                "        test_path = f'../../data/{name}_test.csv'\n",
                "        \n",
                "        if os.path.exists(train_path):\n",
                "            train_df = pd.read_csv(train_path)\n",
                "            test_df = pd.read_csv(test_path) if os.path.exists(test_path) else None\n",
                "            \n",
                "            datasets[name] = {\n",
                "                'train': train_df,\n",
                "                'test': test_df\n",
                "            }\n",
                "            print(f\"âœ“ Loaded {name.upper()} dataset: {len(train_df)} train samples\")\n",
                "            if test_df is not None:\n",
                "                print(f\"  Test samples: {len(test_df)}\")\n",
                "        else:\n",
                "            print(f\"âœ— {name.upper()} dataset not found at {train_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"âœ— Error loading {name.upper()}: {e}\")\n",
                "\n",
                "# Use KIBA as example if available, otherwise create sample data\n",
                "if 'kiba' in datasets and datasets['kiba']['train'] is not None:\n",
                "    sample_df = datasets['kiba']['train'].head(100)\n",
                "    print(f\"\\nUsing KIBA dataset sample: {len(sample_df)} entries\")\nelse:\n",
                "    # Create sample data for demonstration\n",
                "    sample_data = {\n",
                "        'compound_iso_smiles': [\n",
                "            'CCO',  # Ethanol\n",
                "            'CC(=O)O',  # Acetic acid\n",
                "            'CC(C)O',  # Isopropanol\n",
                "            'C1=CC=CC=C1',  # Benzene\n",
                "            'CCN(CC)CC',  # Triethylamine\n",
                "        ],\n",
                "        'target_sequence': [\n",
                "            'MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG',\n",
                "            'MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG',\n",
                "            'MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG',\n",
                "            'MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG',\n",
                "            'MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG',\n",
                "        ],\n",
                "        'affinity': [5.2, 6.1, 4.8, 7.3, 5.9]\n",
                "    }\n",
                "    sample_df = pd.DataFrame(sample_data)\n",
                "    print(f\"\\nCreated sample dataset: {len(sample_df)} entries\")\n",
                "\n",
                "print(\"\\nDataset columns:\", list(sample_df.columns))\n",
                "sample_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Quality Analysis\n",
                "\n",
                "Let's analyze the quality of our data and identify potential issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze data quality\n",
                "print(\"=== Data Quality Analysis ===\")\n",
                "print(f\"Total samples: {len(sample_df)}\")\n",
                "print(f\"Missing values:\")\n",
                "print(sample_df.isnull().sum())\n",
                "print()\n",
                "\n",
                "# SMILES validation\n",
                "valid_smiles = []\n",
                "invalid_smiles = []\n",
                "\n",
                "for idx, smiles in enumerate(sample_df['compound_iso_smiles']):\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol is not None:\n",
                "            valid_smiles.append(idx)\n",
                "        else:\n",
                "            invalid_smiles.append(idx)\n",
                "    except:\n",
                "        invalid_smiles.append(idx)\n",
                "\n",
                "print(f\"SMILES validation:\")\n",
                "print(f\"  Valid: {len(valid_smiles)} ({len(valid_smiles)/len(sample_df)*100:.1f}%)\")\n",
                "print(f\"  Invalid: {len(invalid_smiles)} ({len(invalid_smiles)/len(sample_df)*100:.1f}%)\")\n",
                "print()\n",
                "\n",
                "# Protein sequence analysis\n",
                "protein_lengths = [len(seq) for seq in sample_df['target_sequence']]\n",
                "print(f\"Protein sequence lengths:\")\n",
                "print(f\"  Min: {min(protein_lengths)} residues\")\n",
                "print(f\"  Max: {max(protein_lengths)} residues\")\n",
                "print(f\"  Mean: {np.mean(protein_lengths):.1f} residues\")\n",
                "print(f\"  Std: {np.std(protein_lengths):.1f} residues\")\n",
                "print()\n",
                "\n",
                "# Affinity distribution\n",
                "print(f\"Affinity values:\")\n",
                "print(f\"  Min: {sample_df['affinity'].min():.2f}\")\n",
                "print(f\"  Max: {sample_df['affinity'].max():.2f}\")\n",
                "print(f\"  Mean: {sample_df['affinity'].mean():.2f}\")\n",
                "print(f\"  Std: {sample_df['affinity'].std():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Molecular Property Analysis\n",
                "\n",
                "Let's analyze the molecular properties of our compounds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate molecular properties\n",
                "properties = []\n",
                "\n",
                "for smiles in sample_df['compound_iso_smiles']:\n",
                "    try:\n",
                "        mol = Chem.MolFromSmiles(smiles)\n",
                "        if mol is not None:\n",
                "            props = {\n",
                "                'smiles': smiles,\n",
                "                'molecular_weight': Descriptors.MolWt(mol),\n",
                "                'logp': Descriptors.MolLogP(mol),\n",
                "                'num_atoms': mol.GetNumAtoms(),\n",
                "                'num_bonds': mol.GetNumBonds(),\n",
                "                'num_rings': rdMolDescriptors.CalcNumRings(mol),\n",
                "                'tpsa': Descriptors.TPSA(mol),\n",
                "                'hbd': Descriptors.NumHDonors(mol),\n",
                "                'hba': Descriptors.NumHAcceptors(mol)\n",
                "            }\n",
                "            properties.append(props)\n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {smiles}: {e}\")\n",
                "\n",
                "props_df = pd.DataFrame(properties)\n",
                "print(f\"Calculated properties for {len(props_df)} compounds\")\n",
                "props_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize molecular properties\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "properties_to_plot = ['molecular_weight', 'logp', 'num_atoms', 'tpsa', 'hbd', 'hba']\n",
                "property_labels = ['Molecular Weight', 'LogP', 'Number of Atoms', 'TPSA', 'H-Bond Donors', 'H-Bond Acceptors']\n",
                "\n",
                "for i, (prop, label) in enumerate(zip(properties_to_plot, property_labels)):\n",
                "    if prop in props_df.columns:\n",
                "        axes[i].hist(props_df[prop], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
                "        axes[i].set_title(label)\n",
                "        axes[i].set_xlabel(label)\n",
                "        axes[i].set_ylabel('Frequency')\n",
                "    else:\n",
                "        axes[i].text(0.5, 0.5, f'{label}\\nNo data', ha='center', va='center', transform=axes[i].transAxes)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing\n",
                "\n",
                "Now let's preprocess the data for model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize data processor\n",
                "processor = DataProcessor(\n",
                "    max_protein_length=200,  # Truncate proteins to 200 residues\n",
                "    validate_smiles=True,\n",
                "    remove_invalid=True\n",
                ")\n",
                "\n",
                "print(\"Processing data...\")\n",
                "processed_data = processor.process_dataframe(sample_df)\n",
                "\n",
                "print(f\"Original samples: {len(sample_df)}\")\n",
                "print(f\"Processed samples: {len(processed_data)}\")\n",
                "print(f\"Removed samples: {len(sample_df) - len(processed_data)}\")\n",
                "\n",
                "# Show processed data structure\n",
                "if len(processed_data) > 0:\n",
                "    sample_item = processed_data[0]\n",
                "    print(\"\\nProcessed data structure:\")\n",
                "    for key, value in sample_item.items():\n",
                "        if hasattr(value, 'shape'):\n",
                "            print(f\"  {key}: {type(value).__name__} {value.shape}\")\n",
                "        else:\n",
                "            print(f\"  {key}: {type(value).__name__} - {str(value)[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Creating Custom Datasets\n",
                "\n",
                "Learn how to create custom datasets for specific use cases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a custom dataset\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Create dataset from processed data\n",
                "dataset = DTADataset(processed_data)\n",
                "\n",
                "print(f\"Created dataset with {len(dataset)} samples\")\n",
                "\n",
                "# Create data loader\n",
                "dataloader = DataLoader(\n",
                "    dataset, \n",
                "    batch_size=2, \n",
                "    shuffle=True,\n",
                "    collate_fn=dataset.collate_fn\n",
                ")\n",
                "\n",
                "print(f\"Created dataloader with batch size 2\")\n",
                "\n",
                "# Test the dataloader\n",
                "print(\"\\nTesting dataloader...\")\n",
                "for i, batch in enumerate(dataloader):\n",
                "    print(f\"Batch {i+1}:\")\n",
                "    for key, value in batch.items():\n",
                "        if hasattr(value, 'shape'):\n",
                "            print(f\"  {key}: {value.shape}\")\n",
                "        elif isinstance(value, list):\n",
                "            print(f\"  {key}: list of {len(value)} items\")\n",
                "        else:\n",
                "            print(f\"  {key}: {type(value).__name__}\")\n",
                "    \n",
                "    if i >= 1:  # Only show first 2 batches\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Data Augmentation\n",
                "\n",
                "Explore data augmentation techniques to improve model robustness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data augmentation examples\n",
                "from unified_dta.data import DataAugmenter\n",
                "\n",
                "# Initialize augmenter\n",
                "augmenter = DataAugmenter(\n",
                "    smiles_augmentation=True,\n",
                "    protein_augmentation=True,\n",
                "    noise_level=0.1\n",
                ")\n",
                "\n",
                "# Example SMILES augmentation (canonical vs random)\n",
                "original_smiles = \"CCO\"\n",
                "print(f\"Original SMILES: {original_smiles}\")\n",
                "\n",
                "# Generate augmented versions\n",
                "augmented_smiles = augmenter.augment_smiles(original_smiles, n_variants=3)\n",
                "print(\"Augmented SMILES:\")\n",
                "for i, smi in enumerate(augmented_smiles):\n",
                "    print(f\"  {i+1}. {smi}\")\n",
                "\n",
                "print()\n",
                "\n",
                "# Protein sequence augmentation (adding noise)\n",
                "original_protein = \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
                "print(f\"Original protein: {original_protein[:30]}...\")\n",
                "\n",
                "augmented_proteins = augmenter.augment_protein(original_protein, n_variants=2)\n",
                "print(\"Augmented proteins:\")\n",
                "for i, prot in enumerate(augmented_proteins):\n",
                "    print(f\"  {i+1}. {prot[:30]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Data Splitting Strategies\n",
                "\n",
                "Learn about different data splitting strategies for DTA prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from unified_dta.data import DTASplitter\n",
                "\n",
                "# Initialize splitter\n",
                "splitter = DTASplitter()\n",
                "\n",
                "# Random split (standard)\n",
                "train_data, val_data, test_data = splitter.random_split(\n",
                "    processed_data, \n",
                "    train_ratio=0.7, \n",
                "    val_ratio=0.15, \n",
                "    test_ratio=0.15,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "print(\"Random Split:\")\n",
                "print(f\"  Train: {len(train_data)} samples\")\n",
                "print(f\"  Validation: {len(val_data)} samples\")\n",
                "print(f\"  Test: {len(test_data)} samples\")\n",
                "print()\n",
                "\n",
                "# Protein-based split (no protein overlap between sets)\n",
                "try:\n",
                "    train_prot, val_prot, test_prot = splitter.protein_split(\n",
                "        processed_data,\n",
                "        train_ratio=0.7,\n",
                "        val_ratio=0.15,\n",
                "        test_ratio=0.15,\n",
                "        random_state=42\n",
                "    )\n",
                "    \n",
                "    print(\"Protein-based Split:\")\n",
                "    print(f\"  Train: {len(train_prot)} samples\")\n",
                "    print(f\"  Validation: {len(val_prot)} samples\")\n",
                "    print(f\"  Test: {len(test_prot)} samples\")\n",
                "except Exception as e:\n",
                "    print(f\"Protein-based split not possible with current data: {e}\")\n",
                "\n",
                "print()\n",
                "\n",
                "# Drug-based split (no drug overlap between sets)\n",
                "try:\n",
                "    train_drug, val_drug, test_drug = splitter.drug_split(\n",
                "        processed_data,\n",
                "        train_ratio=0.7,\n",
                "        val_ratio=0.15,\n",
                "        test_ratio=0.15,\n",
                "        random_state=42\n",
                "    )\n",
                "    \n",
                "    print(\"Drug-based Split:\")\n",
                "    print(f\"  Train: {len(train_drug)} samples\")\n",
                "    print(f\"  Validation: {len(val_drug)} samples\")\n",
                "    print(f\"  Test: {len(test_drug)} samples\")\n",
                "except Exception as e:\n",
                "    print(f\"Drug-based split not possible with current data: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Data Export and Saving\n",
                "\n",
                "Save processed data for future use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import json\n",
                "\n",
                "# Save processed data\n",
                "output_dir = \"processed_data\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Save as pickle (preserves all data types)\n",
                "with open(f\"{output_dir}/processed_data.pkl\", \"wb\") as f:\n",
                "    pickle.dump(processed_data, f)\n",
                "print(f\"âœ“ Saved processed data to {output_dir}/processed_data.pkl\")\n",
                "\n",
                "# Save splits\n",
                "splits = {\n",
                "    'train': train_data,\n",
                "    'val': val_data,\n",
                "    'test': test_data\n",
                "}\n",
                "\n",
                "for split_name, split_data in splits.items():\n",
                "    with open(f\"{output_dir}/{split_name}_data.pkl\", \"wb\") as f:\n",
                "        pickle.dump(split_data, f)\n",
                "    print(f\"âœ“ Saved {split_name} split: {len(split_data)} samples\")\n",
                "\n",
                "# Save metadata\n",
                "metadata = {\n",
                "    'total_samples': len(processed_data),\n",
                "    'train_samples': len(train_data),\n",
                "    'val_samples': len(val_data),\n",
                "    'test_samples': len(test_data),\n",
                "    'max_protein_length': 200,\n",
                "    'processing_date': pd.Timestamp.now().isoformat()\n",
                "}\n",
                "\n",
                "with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "print(f\"âœ“ Saved metadata to {output_dir}/metadata.json\")\n",
                "\n",
                "print(f\"\\nAll processed data saved to '{output_dir}/' directory\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this tutorial, you learned how to:\n",
                "\n",
                "1. **Load standard datasets** (KIBA, Davis, BindingDB)\n",
                "2. **Analyze data quality** and identify issues\n",
                "3. **Calculate molecular properties** for compounds\n",
                "4. **Preprocess data** for model training\n",
                "5. **Create custom datasets** and data loaders\n",
                "6. **Apply data augmentation** techniques\n",
                "7. **Use different splitting strategies** (random, protein-based, drug-based)\n",
                "8. **Save and export** processed data\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "- **Model Training**: See `03_model_training.ipynb` to learn how to train models with your processed data\n",
                "- **Advanced Configuration**: Explore `04_advanced_configuration.ipynb` for custom model architectures\n",
                "- **Performance Optimization**: Check `05_performance_optimization.ipynb` for memory and speed optimization\n",
                "\n",
                "## ðŸ’¡ Tips for Success\n",
                "\n",
                "- Always validate your SMILES strings before training\n",
                "- Consider protein sequence length limits for memory efficiency\n",
                "- Use appropriate splitting strategies based on your research goals\n",
                "- Save processed data to avoid recomputation\n",
                "- Monitor data quality throughout your pipeline"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}